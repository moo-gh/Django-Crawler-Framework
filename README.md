# Websites Crawler Framework

A powerful Django-based web crawling framework designed to monitor websites for new content and automatically notify users of updates. Perfect for real estate monitoring, news aggregation, job hunting, and any scenario where being first to discover new content provides an advantage.

## ğŸ¯ Use Cases

- **Real Estate Monitoring**: Be the first to apply to new property listings
- **News Aggregation**: Get notified of new articles from your favorite websites
- **Job Hunting**: Monitor job boards for new opportunities
- **E-commerce**: Track new products or price drops
- **Content Monitoring**: Stay updated on blogs, forums, and social media

## ğŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose
- Git

### Installation

1. **Clone the repository**
   ```bash
   git clone git@github.com:ghorbani-mohammad/crawler-framework.git
   cd crawler-framework
   ```

2. **Start the application**
   ```bash
   # Development
   docker-compose up
   
   # Production
   docker-compose -f docker-compose-prod.yml up
   ```

3. **Access the application**
   - Django Admin: `http://localhost:8000/secret-admin/`
   - API Endpoints: `http://localhost:8000/api/`

## ğŸ—ï¸ Architecture Overview

This framework is built around three core entities that work together to create a comprehensive crawling solution:

### 1. Agency
**Definition**: Represents a website or domain you want to monitor
**Examples**: CNN, BBC, Digikala, LinkedIn Jobs

**Key Features**:
- Configure website-specific settings
- Set crawling intervals and timeouts
- Define proxy settings
- Configure notification preferences

### 2. Page
**Definition**: Specific pages within an agency that contain the content you want to monitor
**Examples**: CNN Politics page, BBC Technology section, Job listings page

**Key Features**:
- Define page-specific crawling rules
- Set up filtering and token management
- Configure scroll behavior for dynamic content
- Set message templates for notifications

### 3. Structure
**Definition**: Defines how to extract data from pages using CSS selectors and XPath
**Purpose**: Tells the crawler engine exactly what elements to look for and how to extract information

**Key Components**:
- **News Links Structure**: CSS selectors to find article/product links
- **Content Structure**: How to extract titles, descriptions, dates, etc.
- **Pagination Structure**: How to navigate through multiple pages

## ğŸ“Š Example: Crawling Job Listings

Here's how you would set up monitoring for a job board:

1. **Create Agency**: "Tech Jobs Board"
2. **Create Page**: "Software Engineer Listings"
3. **Define Structure**: 
   - News Links: `a.job-listing-link`
   - Title: `h2.job-title`
   - Company: `span.company-name`
   - Location: `span.job-location`

## ğŸ”§ Configuration

### Environment Variables
```bash
# Database
DATABASE_URL=postgresql://user:pass@localhost/db

# Email (for notifications)
EMAIL_HOST=smtp.gmail.com
EMAIL_PORT=587
EMAIL_HOST_USER=your-email@gmail.com
EMAIL_HOST_PASSWORD=your-app-password

# Selenium Grid
SELENIUM_HUB_URL=http://selenium-hub:4444/wd/hub

# Redis (for Celery)
REDIS_URL=redis://redis:6379/0
```

### Key Features

#### ğŸ” Multi-Browser Support
- Uses Selenium Grid for parallel browser sessions
- Supports Chrome, Firefox, Safari
- Handles JavaScript-heavy websites

#### ğŸ“§ Notification System
- Email notifications for new content
- Telegram bot integration
- Customizable message templates

#### ğŸ›¡ï¸ Proxy Support
- Rotate proxies to avoid rate limiting
- Configure per-agency proxy settings
- Automatic proxy health checking

#### ğŸ“ˆ Monitoring & Logging
- Comprehensive logging in Django admin
- Performance metrics and crawl statistics
- Error tracking and alerting

#### â° Scheduling
- Flexible scheduling with cron-like syntax
- Off-time configuration to avoid peak hours
- Per-page scheduling options

## ğŸ‘¥ Guest Access

Want to see the framework in action? Use our guest account to explore the admin panel and see real-world examples:

- **URL**: [https://crawler.m-gh.com/secret-admin/](https://crawler.m-gh.com/secret-admin/)
- **Username**: `guest`
- **Password**: `RPxzsoen4O`

The guest account provides read-only access to see how various websites are configured and how the framework handles different types of content.

## ğŸ“± Telegram Channels

See the framework in action through these Telegram channels that are powered by this crawler:

- [The New Yorker Agency News](https://t.me/newyorkercom)
- [Product Hunt Daily](https://t.me/producthuntdaily)
- [Python Jobs](https://t.me/iran_careers)
- [More channels...]

## ğŸ› ï¸ Management Commands

Use the provided shell script for easy management:

```bash
# Start the application
./mng-api.sh start

# Stop the application
./mng-api.sh stop

# View logs
./mng-api.sh logs

# Restart services
./mng-api.sh restart
```

## ğŸ“ Project Structure

```
crawler/
â”œâ”€â”€ agency/                 # Main crawling app
â”‚   â”œâ”€â”€ models.py          # Agency, Page, Structure models
â”‚   â”œâ”€â”€ views.py           # API endpoints
â”‚   â”œâ”€â”€ tasks.py           # Celery tasks
â”‚   â””â”€â”€ crawler_engine.py  # Core crawling logic
â”œâ”€â”€ notification/           # Notification system
â”œâ”€â”€ reusable/              # Shared utilities
â”œâ”€â”€ crawler/               # Django settings
â””â”€â”€ manage.py              # Django management
```

## ğŸ”„ Development

### Running Tests
```bash
docker-compose exec web python manage.py test
```

### Code Formatting
```bash
# Format Python code
make format-python

# Lint Python code
make lint-python
```

### Database Migrations
```bash
docker-compose exec web python manage.py makemigrations
docker-compose exec web python manage.py migrate
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

If you encounter any issues or have questions:

1. Check the [Issues](https://github.com/ghorbani-mohammad/crawler-framework/issues) page
2. Review the logs in Django admin
3. Check the documentation in the code comments

---

**Built with â¤ï¸ using Django, Celery, Selenium, and Docker**
